# Python Experiments

This repository contains a set of projects and scripts with the purpose to evaluate the quality of test sets generated by [Pynguin](https://pypi.org/project/pynguin/) automatic test generator. At all, Pynguin has five different algorithms for test set generation: DYNAMOSA, MIO, MOSA, WHOLE_SUITE, and RANDOM.

The idea of this project is to evaluate the quality of the test set generated by each algorithm agains a set of mutants generated by four different mutant generation tool for Python: [Mut.Py](https://pypi.org/project/MutPy/), [MutMut](https://pypi.org/project/mutmut/), [Mutatest](https://pypi.org/project/mutatest/), and [Cosmic.Ray](https://pypi.org/project/cosmic-ray/).

There are also scripts to evaluate the quality of different combinations of test sets.

## Scripts execution

These scripts are responsible to run and collect the experimentation data from different testing tools. Below we explain the order and parameters used to call each one of them.

In general, we have two kinds of scripts. The one which runs on a single project and the other which runs over a set of programs, calling the scripts for each individual program.

Additionally, all the scripts demands a parameter indicating the so-called `<project root dir>`, which corresponds to the complete path until the projects under testing.

### How to specify the CSV file indicating the projects under testing

From the `<project root dir>`, we need to specify the set of programs under testing. We do it by providing for some scripts a text file with a content similar to the one below:

```
bfs:breadth_first_search.py:search
binheap:binheap.py:data-structure
bst1:binary_search_tree.py:data-structure
bst1:bst2.py:data-structure
...
```

It is a comma separated value, actually we are using colon (`:`) as field separator.

The two first fields are mandatory, they represent the name of the directory containing the project ander testing, and the name of the module under testing. Other fields are optional. For instance, considering the first line of the file above:

```
bfs:breadth_first_search.py:search
```

First field `bsf` means that, from the `<project root dir>`, we have a directory called `bsf` and inside this directory we have a file named `breadth_first_search.py`. In this particular example, we also have a third field, `search`, which indicates the kind of project we are dealing with.

In the current project, we have a file, named `files.txt` with the content specified above.

### Static Metrics

Using the script `00metricsCalculation.py`, we can collect a set of static metrics for each project. The scripts requires two mandatory parameter: `<project root dir>` and `<test-set-file>`. From the `<project root dir>` it looks for `<test-set-file>` and calculates the set of metrics for each Python project listed on `<test-set-file>`.

After execution, the scripts generates a file named `report-radon-metrics.csv` inside `<project root dir>` directory.

### Pynguin test sets for each algorithm

Utilizing the script `00runPynguin.sh`, we can execute the automatic test set generator with different algorithms.

There are a few mandatory parameters, such as: `<project root dir>`, `<algorithm>` and `<seed>`. It is also possible to include the optional paramenter `<max_timeout(s)>`.

The five possible algorithms for test generation are: DYNAMOSA, MIO, MOSA, WHOLE_SUITE and RANDOM.

After execution, a folder `./algorithm` will be created inside the corresponding project root directory file as indicated in `files.txt`. This folder will include a test set file. 

### Correct Pynguin test set

It is necessary to correct the Pynguin generated test sets so it can be correctly processed by the mutation tools.

For this, the `01parsePynguin.sh` script must be executed with the mandatory parameters `<project root dir>` and `<algorithm>`.

As a way to make this process faster, the `01parsePynguinRunALL.sh` script can be executed with all the algorithms listed before, only needing the `<project root dir>` parameter.

### Validate Test Set

So that we can verify that all the generated test sets are working as expected, the `02validateTestSet.sh` script must be executed. It will run all the tests located in `<project root dir>`.

In this script, at least one algorithm can be passed as a parameter, but it also accepts multiple at the same time.

### Coverage measure

With all the tests generated, `03coverageReport.sh` can be run to generate coverage data of the test sets and then `04coverageSummary.py` can be run to generate a csv report of the coverage metrics of all of the programs included in `filex.txt`.

It recieves the mandatory parameters `<project root dir>`, `<data-file>` and `<test-set-file>`, the later usually being the name of the algorithm used to generate the test set.

### Evaluate test sets on mutation tools

The next step is to run all of the previously mentioned mutation tools on the generated test sets. 

So that can be possible, there is one script for each tool: `<05evalTestOnMutPy.sh>`, `<05evalTestOnMutMut.sh>`, `<05evalTestOnMutatest.sh>` and `<05evalTestOnCosmicRay.sh>`.

All of them require the mandatory parameters `<project root dir>` and `<test case directory>`.

The generated output is a set of mutants inside the `test case directory/mutation tool`.

### Merge test set

So it is possible to evaluate the influence of the different test generating algorithm on each other, `06mergeTestSet.sh` can be executed to combine different test sets in to one file, so it can be later evaluated with the mutation tools. 

It requires the mandatory parameters `<project root dir>` and `<list of test sets directory to be merged>`.

As there are new tests set generated, return to Coverage Measure to evaluate the mutation tools and coverage reports.

### Generate Summary Reports
 
With all the desired combinations of test sets generated and evaluated, it is necessary to compile all the generated information of the mutation tools in easy-to-read csv files.

For that to be possible, four different scripts to collect mutation information according to the tool were created: `<07mutpySummary.py>`, `<07mutmutSummary.py>`, `<07mutatestSummary.py>` and `<07cosmicRaySummary.py>`.

The mandatory parameters are `<project root dir>`, `<data-file>` and `<test-set>`. The output is written in a csv file located in `<project root dir>`.
 
It is also possible to collect the time data to compile it on another csv file utilizing `<07timeSummary.py>`. The parameters are the same as above but with one extra: `<mutation-tool>`.

### Compiling Summary Reports

With all of the summary reports generated, it is possible to compile all of the data into one CSV file so it can be easily interpreted by R scripts.

The scripts that collect time, coverage, mutation score and size of the set of mutants are `<08timeCompiled.py>`, `<08coverageCompiled.py>`, `<08mutationScoreCompiled.py>` and `<08sizeCompiled.py>`, respectively.

The mandatory parameters for time, mutation score and size are `<project root dir>`, `<test-sets file name>` and `<mutation-tool-file>`.

For coverage, they are `<project root dir>`, `<test-sets file name>` and  `<type of coverage>`

### Acessing Cosmic-Ray database

Use `sqlitebrowser` to navegate over `.sqlite` database generated by Cosmic-Ray, then, we may prepare SQL statements to get information that we want.

```
sudo apt  install sqlitebrowser sqlite3 
```

Fos instance, SQL below list the total number of generated mutants per mutation operator:

```
SELECT count(*) FROM mutation_specs
SELECT operator_name, count(*) FROM mutation_specs GROUP by operator_name
```

The script `my-cr-report.py` uses Python to access such a database file.

```
SELECT count(*) FROM mutation_specs as MS, work_results as WR WHERE MS.job_id = WR.job_id and WR.test_outcome is "KILLED"

SELECT MS.operator_name, count(*) FROM mutation_specs as MS, work_results as WR WHERE MS.job_id = WR.job_id and WR.test_outcome is "KILLED" GROUP by MS.operator_name

```

