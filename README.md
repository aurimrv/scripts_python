# Python Experiments

This repository contains a set of projects and scripts with the purpose to evaluate the quality of test sets generated by [Pynguin](https://pypi.org/project/pynguin/) automatic test generator. At all, Pynguin has five different algorithms for test set generation: DYNAMOSA, MIO, MOSA, WHOLE_SUITE, and RANDOM.

The idea of this project is to evaluate the quality of the test set generated by each algorithm agains a set of mutants generated by four different mutant generation tool for Python: [Mut.Py](https://pypi.org/project/MutPy/), [MutMut](https://pypi.org/project/mutmut/), [Mutatest](https://pypi.org/project/mutatest/), and [Cosmic.Ray](https://pypi.org/project/cosmic-ray/).

There are also scripts to evaluate the quality of different combinations of test sets.

## Scripts execution

These scripts are responsible to run and collect the experimentation data from different testing tools. Below we explain the order and parameters used to call each one of them.

In general, we have two kinds of scripts. The one which runs on a single project and the other which runs over a set of programs, calling the scripts for each individual program.

Additionally, all the scripts demands a parameter indicating the so-called `<project root dir>`, which corresponds to the complete path until the projects under testing.

### How to specify the CSV file indicating the projects under testing

From the `<project root dir>`, we need to specify the set of programs under testing. We do it by providing for some scripts a text file with a content similar to the one below:

```
bfs:breadth_first_search.py:search
binheap:binheap.py:data-structure
bst1:binary_search_tree.py:data-structure
bst1:bst2.py:data-structure
...
```

It is a comma separated value, actually we are using colon (`:`) as field separator.

The two first fields are mandatory, they represent the name of the directory containing the project ander testing, and the name of the module under testing. Other fields are optional. For instance, considering the first line of the file above:

```
bfs:breadth_first_search.py:search
```

First field `bsf` means that, from the `<project root dir>`, we have a directory called `bsf` and inside this directory we have a file named `breadth_first_search.py`. In this particular example, we also have a third field, `search`, which indicates the kind of project we are dealing with.

In the current project, we have a file, named `files.txt` with the content specified above.

### Static Metrics

### Pynguin test sets for each algorithm

Utilizing the script `runPynguin.sh`, we can execute the automatic test set generator with different algorithms.

There are a few mandatory parameters, such as: `<project root dir>`, `<algorithm>` and `<seed>`. It is also possible to include the optional paramenter `<max_timeout(s)>`.

The five possible algorithms for test generation are: DYNAMOSA, MIO, MOSA, WHOLE_SUITE and RANDOM.

After execution, a folder `./algorithm` will be created inside the corresponding project root directory file as indicated in `files.txt`. This folder will include a test set file. 

### Correct Pynguin test set

It is necessary to correct the Pynguin generated test sets so it can be correctly processed by the mutation tools.

For this, the `parsePynguin.sh` script must be executed with the mandatory parameters `<project root dir>` and `<algorithm>`.

As a way to make this process faster, the `parsePynguinRunALL.sh` script can be executed with all the algorithms listed before, only needing the `<project root dir>` parameter.

### Validate Test Set

So that we can verify that all the generated test sets are working as expected, the `validateTestSet.sh` script must be executed. It will run all the tests located in `<project root dir>`.

In this script, at least one algorithm can be passed as a parameter, but it also accepts multiple at the same time.

### Coverage measure

With all the tests generated, `coverageSummary.py` can be run to generate a csv report of the coverage metrics of all of the programs included in `filex.txt`.

It recieves the mandatory parameters `<project root dir>`, `<data-file>` and `<test-set>`, the later usually being the name of the algorithm used to generate the test set.

### Evaluate test sets on mutation tools

The next step is to run all of the previously mentioned mutation tools on the generated test sets. 

So that can be possible, there is one script for each tool: `<evalTestOnMutPy.sh>`, `<evalTestOnMutMut.sh>`, `<evalTestOnMutatest.sh>` and `<evalTestOnCosmicRay.sh>`.

All of them require the mandatory parameters `<project root dir>` and `<test case directory>`.

The generated output is a set of mutants inside the `test case directory/mutation tool`.

### Merge test set

So it is possible to evaluate the influence of the different test generating algorithm on each other, `mergeTestSet.sh` can be executed to combine different test sets in to one file, so it can be later evaluated with the mutation tools. 

It requires the mandatory parameters `<project root dir>` and `<list of test sets directory to be merged>`.

As there are new tests set generated, return to Coverage Measure to evaluate the mutation tools and coverage reports.

### Generate Summary Reports
 
With all the desired combinations of test sets generated and evaluated, it is necessary to compile all the generated information of the mutation tools in easy-to-read csv files.

For that to be possible, four different scripts to collect mutation information according to the tool were created: `<mutpySummary.sh>`, `<mutmutSummary.sh>`, `<mutatestSummary.sh>` and `<cosmicRaySummary.sh>`.

The mandatory parameters are `<project root dir>`, `<data-file>` and `<test-set>`. The output is written in a csv file located in `<project root dir>`.
 
It is also possible to collect the time data and compile it in another csv file utilizing `<timeSummary.sh>`. The parameters are the same as above but with one extra: `<mutation-tool>`.